{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<h1 style=\"text-align: center;color:orange\" markdown=\"1\"> DEFI IA  </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"color:blue\"> CONTENU </h2>\n",
    " \n",
    " \n",
    " > I. Présentation des méthodes utilisées: **Explication de l'approche** <br>\n",
    "     \n",
    " > II.Meilleur soumission: <br>\n",
    "     >> II.1 Présentation de l'approche<br>\n",
    "     >> II.2 Bliblothèques utilisées  <br>\n",
    "     >> II.3 présentation du code  <br>\n",
    "     >> II.4 Evaluation des métrics <br>\n",
    "\n",
    " > CONCLUSION <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">I. Notre Expérience dans le challenge: **Explication de l'approche** </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Approche Classique**\n",
    "<br> Nous nous trouvons en face de données textuelles, l'approche classique consiste à transformer le corpus de mots qui est sur un format non structuré dont non adapté pour les modèles de machine learning, pour ce faire nous transformons nos données en données structurées au format compréhensible pour nos modèles de machine learning (**SVM,RandomForest,Naives Bayes,Logistic Regression,XGboost,lightbm**).\n",
    "Avant la phase de transformation des données en format structuré, les données ont subi un certain nombres de traitements qui sont entre autres:\n",
    "* La suppression des bruits du corpus telles que : **sites web**,**emails**,**mots non alphanumériques**.\n",
    "- Méthodes de transformation des données en format structuré utiliser :\n",
    "* Matrice Tfidf avec utilisations des ngrams\n",
    "* Création des words vectors en  se basant sur des modèles pré entrainés à l'aide des bilblothèques telles que : **word2vec**,**glove**.\n",
    "\n",
    "Pour la gestion du problème de classes déséquilibrées nous avions opté pour la méthode de ré-equilibrage\n",
    "**SMOTE**\n",
    "**Résultats obtenu**: Après utilisations de toutes ses méthodes classiques, notre meilleur score fut **75.67%**\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - **Approche moderne (Etat de l'art en NLP)**\n",
    "<br> Nous nous sommes tournés également vers les méthodes modernes en  se basant sur les transformateurs\n",
    "de Hugging Face, l'une des bibliothèques de la PNL qui connaît le plus grand succès actuellement.\n",
    "avec le méchanisme de transfert learning. Pour ce faire nous utilisons la biblothèque **Ktrain** basé sur \n",
    "**Tensorflow** et **Keras** avec le modèle pré-entrainé **distilbert**  qui est une distribution de **Bert**. \n",
    "\n",
    "**Pourquoi **Ktrain** et **distilbert**?**:\n",
    "**Ktrain :**Nous choisissons cette biblothèque **Ktrain** pour sa flexibilité et sa mise en oeuvre.\n",
    "**distilbert :**Nous optons pour ce modèle car nous l'avons trouvé moins lourd pour la phase d'entrainement\n",
    "plus ou moins lent.\n",
    "</br>\n",
    "**Meilleur résultat obtenu**: **79.96%**\n",
    "\n",
    "<br> Pour ameliorer les performances de notre algorithme de deep learning, nous nous camptons sur les \n",
    "classes ayant les taux de précisions les plus faibles puis faisons une Data augmentation pour accroitre\n",
    "le nombre d'individus (de descriptions ) présent dans ces classes à l'aide de la méthode **backtranslate**\n",
    "qui consiste à traduire le texte en une langue donnée puis le retraduire dans sa forme initiale.\n",
    "</br>\n",
    "**Résultat obtenu**: **80.10%** (Notre meilleur score sur le leader bord public)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">II.Meilleur Soumission </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">II.1 Présentation de l'approche </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre approche pour cette soumission a été essentiellement basé sur l'utilisation des langages models basés sur des transformeurs notamment **distil-bert** car celui-ci a été pour nous le modèle pré-entrainé le plus facile à entrainer. Pour accroitre le pouvoir prédicteurs de notre classifieur basé sur les langages models, nous récensons les classes ayant un taux de prédictions faibles en validation-test puis améliorons la prédictions de ses classes en se basant sur une technique de data-augmentation notamment **backtranslate** ce qui nous permet d'accroitre le nombre d'individus des classes mal prédites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">II.2 Bliblothèques utilisées </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ktrain\n",
    "# version 0.23.2\n",
    "! pip install transformers\n",
    "! pip install mosestokenizer\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">II.3 présentation du code</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des données\n",
    "path=\"C:/Users/konate/Desktop/IA_kaagle/Data\"\n",
    "train_set = pd.read_json(path+\"/train.json\")\n",
    "labels = pd.read_csv(path+\"/train_label.csv\")\n",
    "categories = pd.read_csv(path+\"/categories_string.csv\", names=[\"Libelle\", \"Category\"], header=0)\n",
    "test_set = pd.read_json(path+\"/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = train_set.join(labels.set_index(\"Id\"), on = \"Id\")\n",
    "df_train = df_train.join(categories.set_index(\"Category\"), on = \"Category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation pour améliorer le pouvoir prédictif de notre classifieur sur les classes mal prédites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download data for a language\n",
    "def download(model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# download model for English -> Romance\n",
    "tmp_lang_tokenizer, tmp_lang_model = download('Helsinki-NLP/opus-mt-en-ROMANCE')\n",
    "# download model for Romance -> English\n",
    "src_lang_tokenizer, src_lang_model = download('Helsinki-NLP/opus-mt-ROMANCE-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts, model, tokenizer, language):\n",
    "    \"\"\"Translate texts into a target language\"\"\"\n",
    "    # Format the text as expected by the model\n",
    "    formatter_fn = lambda txt: f\"{txt}\" if language == \"en\" else f\">>{language}<< {txt}\"\n",
    "    original_texts = [formatter_fn(txt) for txt in texts]\n",
    "\n",
    "    # Tokenize (text to tokens)\n",
    "    tokens = tokenizer.prepare_seq2seq_batch(original_texts)\n",
    "\n",
    "    # Translate\n",
    "    translated = model.generate(**tokens)\n",
    "\n",
    "    # Decode (tokens to text)\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(texts, language_src, language_dst):\n",
    "    \"\"\"Implements back translation\"\"\"\n",
    "    # Translate from source to target language\n",
    "    translated = translate(texts, tmp_lang_model, tmp_lang_tokenizer, language_dst)\n",
    "\n",
    "    # Translate from target language back to source language\n",
    "    back_translated = translate(translated, src_lang_model, src_lang_tokenizer, language_src)\n",
    "\n",
    "    return back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df1.loc[:,'description']=back_translate(df1.loc[:,'description'].tolist(),\"en\",\"fr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,df1],axis=0)\n",
    "df=df.drop(columns=['Unnamed: 0'],axis=1)\n",
    "df.dropna(inplace=True)\n",
    "df.Category=df.Category.astype(int)\n",
    "df_train=pd.concat([df_train.drop(columns=['Id'],axis=1),df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Echantillonage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "(173998, 5)\n",
      "(43435, 5)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "y, categories = df_train['Category'].factorize()\n",
    "df_train['category_id'] = y\n",
    "categories = categories.to_list()\n",
    "# train test split\n",
    "train_df=df_train.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test_df=df_train.drop(train_df.index)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391fa03270f04655b2ab468f00b0a43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Choix du modèle pré-entrainés distilbert comportant 66M de parametres \n",
    "model_name = 'distilbert-base-uncased'\n",
    "trans = ktrain.text.Transformer(model_name, maxlen=512, class_names=categories) #maxlen = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 61\n",
      "\t95percentile : 116\n",
      "\t99percentile : 134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6223a51a74444da8e4721261a0ad6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8834acf92b2c4c2e83054acb8fb27e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 61\n",
      "\t95percentile : 116\n",
      "\t99percentile : 134\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocessing à l'aide de ktrain\n",
    "train_data = trans.preprocess_train(train_df.description.values, train_df.category_id.values)\n",
    "test_data = trans.preprocess_test(test_df.description.values, test_df.category_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9dbc287e4546c2a886225510204377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#choisir un classifieur\n",
    "model = trans.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=train_data, val_data=test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #rechercher le meilleur learning rate (meilleur pas de la descente de gradient)\n",
    "learner.lr_find(show_plot=True, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nous formons notre transformers en utilisant l'un des trois taux d'apprentissage recommandés dans l'article BERT: 5e-5, 3e-5 ou 2e-5. Alternativement, l'outil de recherche de taux d'apprentissage ktrain mis dans le chunck précédent  peut être utilisé pour trouver un bon taux d'apprentissage en appelant Learner.lr_find () et Learner.lr_plot (),vu le temps d'entrainement long du classifieur nous optons pour 2 epochs.A voir https://arxiv.org/pdf/1803.09820.pdf*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 3e-05...\n",
      "Epoch 1/2\n",
      "10875/10875 [==============================] - 6240s 574ms/step - loss: 0.8003 - accuracy: 0.7830 - val_loss: 0.5765 - val_accuracy: 0.8253\n",
      "Epoch 2/2\n",
      "10875/10875 [==============================] - 6236s 573ms/step - loss: 0.4508 - accuracy: 0.8644 - val_loss: 0.4860 - val_accuracy: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec5825c910>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choix du learning rate et du nombre de epocs pour la phase d'apprentissage\n",
    "learner.fit_onecycle(3e-5, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "id:41563 | loss:11.82 | true:7 | pred:3)\n",
      "\n",
      "----------\n",
      "id:20103 | loss:11.17 | true:16 | pred:19)\n",
      "\n",
      "----------\n",
      "id:34485 | loss:10.52 | true:4 | pred:6)\n",
      "\n",
      "----------\n",
      "id:2979 | loss:10.51 | true:12 | pred:22)\n",
      "\n",
      "----------\n",
      "id:25126 | loss:10.26 | true:7 | pred:19)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vérifier les 5 classes dans lequelles ont se trompent le plus\n",
    "learner.view_top_losses(n=5, preproc=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5 style=\"color:blue\">III.4 Evaluation des métrics</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          19       0.90      0.93      0.92     13947\n",
      "           9       0.83      0.79      0.81       624\n",
      "          24       0.78      0.69      0.73      1175\n",
      "          22       0.82      0.76      0.79      2120\n",
      "           6       0.78      0.82      0.80      2429\n",
      "           3       0.69      0.66      0.67      1799\n",
      "          14       0.90      0.85      0.87      2509\n",
      "          26       0.90      0.91      0.91      3812\n",
      "          13       0.74      0.77      0.75       850\n",
      "           5       0.84      0.86      0.85       917\n",
      "          11       0.78      0.75      0.77      2329\n",
      "          17       0.81      0.67      0.73       307\n",
      "           4       0.67      0.71      0.69       144\n",
      "          20       0.89      0.91      0.90      2953\n",
      "           8       0.79      0.80      0.80      1302\n",
      "          18       0.88      0.85      0.86       790\n",
      "          27       0.83      0.85      0.84       434\n",
      "          10       0.79      0.84      0.82       165\n",
      "           1       0.86      0.79      0.83       860\n",
      "          25       0.88      0.90      0.89       681\n",
      "          16       0.94      0.95      0.94      1070\n",
      "          15       0.81      0.79      0.80       878\n",
      "          12       0.85      0.80      0.82       318\n",
      "           2       0.74      0.80      0.77       174\n",
      "           7       0.77      0.74      0.76       175\n",
      "           0       0.70      0.68      0.69       319\n",
      "          21       0.87      0.77      0.82       159\n",
      "          23       0.88      0.72      0.79       195\n",
      "\n",
      "    accuracy                           0.85     43435\n",
      "   macro avg       0.82      0.80      0.81     43435\n",
      "weighted avg       0.85      0.85      0.85     43435\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13030,    14,    49,   124,    98,   104,    53,    86,    31,\n",
       "           23,    93,     6,     3,    23,    96,    27,    13,     0,\n",
       "            0,    14,     7,    35,     1,     1,     1,    15,     0,\n",
       "            0],\n",
       "       [   22,   491,    10,     3,    24,    15,     4,    29,     2,\n",
       "            2,     3,     0,     1,     6,     1,     0,     0,     0,\n",
       "            4,     0,     0,     1,     1,     0,     0,     5,     0,\n",
       "            0],\n",
       "       [   73,    10,   806,     4,    11,    17,     0,    22,   154,\n",
       "           10,     1,     0,     0,    21,     4,     0,     1,     2,\n",
       "            5,     4,     1,     5,     0,     0,    21,     2,     1,\n",
       "            0],\n",
       "       [  310,     2,     4,  1608,    27,    48,    32,    18,     2,\n",
       "            4,    16,     2,     4,     7,     6,     1,     6,     1,\n",
       "            1,     1,     2,     6,     1,     5,     1,     5,     0,\n",
       "            0],\n",
       "       [  122,    11,    10,    17,  1989,    44,     9,    43,    14,\n",
       "            7,     6,     1,     4,    66,     3,    12,     6,     7,\n",
       "            9,     2,     2,    22,     8,     1,     4,     7,     2,\n",
       "            1],\n",
       "       [  204,     6,     9,    46,    76,  1182,    20,    31,     9,\n",
       "           18,     9,     2,     8,    24,     3,     6,     2,     3,\n",
       "           20,    15,     1,    43,     8,    23,     2,    28,     0,\n",
       "            1],\n",
       "       [   49,     1,     0,    38,    20,    47,  2122,    18,     0,\n",
       "            2,   143,     2,     4,     7,    11,     0,    20,     0,\n",
       "           11,     0,     1,     4,     0,     2,     1,     6,     0,\n",
       "            0],\n",
       "       [  110,    25,     8,    18,    51,    38,    10,  3479,     7,\n",
       "            1,     3,     1,     1,     4,     3,     5,     1,     1,\n",
       "            9,     3,     0,     7,     1,     0,     2,     7,     0,\n",
       "           17],\n",
       "       [   51,     6,    74,     1,    21,    15,     0,     7,   655,\n",
       "            2,     1,     0,     1,     5,     1,     1,     2,     0,\n",
       "            1,     1,     0,     2,     0,     0,     0,     3,     0,\n",
       "            0],\n",
       "       [   17,     0,     8,     3,     9,     8,     1,     2,     0,\n",
       "          790,     2,     0,     0,    61,     0,     2,     0,     0,\n",
       "            3,     3,     0,     7,     0,     0,     1,     0,     0,\n",
       "            0],\n",
       "       [  224,     3,     2,    36,    11,    23,    63,    21,     2,\n",
       "            1,  1757,    27,     1,     3,   119,     0,    11,     0,\n",
       "            4,     0,    13,     0,     1,     5,     0,     2,     0,\n",
       "            0],\n",
       "       [    2,     1,     1,     3,     0,     0,     0,     2,     2,\n",
       "            0,    70,   207,     4,     0,     7,     0,     3,     0,\n",
       "            1,     0,     3,     0,     0,     1,     0,     0,     0,\n",
       "            0],\n",
       "       [    0,     1,     0,     4,     5,     7,     2,     0,     0,\n",
       "            0,     1,     1,   102,     1,     1,     0,     4,     0,\n",
       "            8,     0,     0,     0,     1,     5,     0,     1,     0,\n",
       "            0],\n",
       "       [   35,     1,    11,     9,    66,    14,     2,     5,     2,\n",
       "           67,     2,     0,     0,  2684,     1,    17,     0,     2,\n",
       "           11,     3,     1,    10,     3,     0,     4,     2,     1,\n",
       "            0],\n",
       "       [   63,     1,     3,     5,     5,     9,     7,     8,     0,\n",
       "            2,   112,     5,     0,     2,  1042,     0,     2,     0,\n",
       "            0,     1,    33,     1,     1,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   11,     0,     2,     1,    40,     4,     0,     1,     3,\n",
       "            5,     2,     0,     0,    36,     0,   669,     1,     0,\n",
       "            0,     4,     0,     4,     7,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   13,     1,     0,     5,     3,     9,    13,     3,     0,\n",
       "            0,     6,     0,     5,     1,     2,     0,   368,     0,\n",
       "            0,     0,     1,     0,     0,     3,     1,     0,     0,\n",
       "            0],\n",
       "       [    0,     1,     0,     1,     5,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     2,     0,     0,     0,   139,\n",
       "            5,     7,     0,     0,     0,     0,     0,     0,     5,\n",
       "            0],\n",
       "       [   20,     4,     7,     6,    20,    29,     8,    12,     5,\n",
       "            1,     4,     0,     7,    27,     2,     3,     1,     1,\n",
       "          683,     4,     1,     1,     4,     2,     0,     4,     4,\n",
       "            0],\n",
       "       [   19,     0,     0,     1,     3,     7,     0,     2,     0,\n",
       "            3,     0,     0,     0,     3,     0,    10,     0,     6,\n",
       "            0,   613,     0,     5,     3,     0,     0,     2,     4,\n",
       "            0],\n",
       "       [   15,     2,     1,     1,     2,     2,     0,     8,     0,\n",
       "            0,     6,     2,     0,     5,     9,     0,     0,     0,\n",
       "            1,     0,  1013,     0,     2,     0,     1,     0,     0,\n",
       "            0],\n",
       "       [   63,     3,     2,     5,    27,    27,     3,     5,     2,\n",
       "            7,     3,     0,     0,     9,     0,     7,     0,     1,\n",
       "            4,     9,     0,   697,     1,     0,     0,     2,     1,\n",
       "            0],\n",
       "       [    3,     0,     2,     2,    17,     9,     2,     6,     1,\n",
       "            0,     0,     0,     2,     2,     0,     2,     0,     1,\n",
       "            6,     2,     0,     6,   254,     0,     0,     0,     1,\n",
       "            0],\n",
       "       [    2,     0,     0,     4,     2,    20,     2,     0,     0,\n",
       "            0,     0,     0,     3,     0,     0,     0,     1,     0,\n",
       "            0,     0,     0,     0,     0,   140,     0,     0,     0,\n",
       "            0],\n",
       "       [    1,     1,    18,     2,     2,     7,     1,     1,     0,\n",
       "            1,     0,     0,     1,     6,     1,     0,     0,     0,\n",
       "            1,     0,     0,     2,     0,     0,   130,     0,     0,\n",
       "            0],\n",
       "       [   36,     2,     4,     3,     9,    28,     2,     7,     0,\n",
       "            0,     1,     1,     0,     1,     1,     0,     0,     0,\n",
       "            2,     1,     0,     3,     1,     0,     0,   216,     0,\n",
       "            1],\n",
       "       [    0,     1,     0,     0,     1,     1,     1,     4,     0,\n",
       "            0,     0,     0,     0,     2,     0,     0,     0,    12,\n",
       "            4,     8,     0,     0,     2,     0,     0,     0,   123,\n",
       "            0],\n",
       "       [    1,     5,     0,     0,     1,     4,     1,    40,     0,\n",
       "            0,     0,     0,     1,     1,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          141]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fonction permettant d'affficher les différentes valeurs du classiffieur pour des metrics et affihage de la matrice de confusion\n",
    "learner.validate(class_names=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score Macro : 81%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.read_csv(\"C:/Users/konate/Desktop/IA_kaagle/seventh_k_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_disparate_impact(people):\n",
    "    counts = people.groupby(['Category', 'gender']).size().unstack('gender')\n",
    "    counts['disparate_impact'] = counts[['M', 'F']].max(axis='columns') / counts[['M', 'F']].min(axis='columns')\n",
    "    return counts['disparate_impact'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred =pred.join(categories.set_index(\"Category\"), on = \"Category\")\n",
    "genders_test = pd.read_json(path+'/test.json').set_index('Id')['gender']\n",
    "test_people = pd.concat((y_pred, genders_test), axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro disparate impact : 4.493004982100444\n"
     ]
    }
   ],
   "source": [
    "print('macro disparate impact :',macro_disparate_impact(test_people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
